{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "BATCH_NORM = True\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train = MNIST(root='data', train=True, transform=transform, download=True)\n",
    "test = MNIST(root='data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, num_workers=2, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=128, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1(nn.Module):\n",
    "    def __init__(self, batch_norm=True):\n",
    "        super(CNN1, self).__init__()\n",
    "        \n",
    "        self.input_dim = None\n",
    "        self.conv1_dim = None\n",
    "        self.conv2_dim = None\n",
    "\n",
    "        # This is a pretty common set up for MNIST\n",
    "        size1 = 16\n",
    "        size2 = 32\n",
    "\n",
    "        conv1 = [\n",
    "            nn.Conv2d(in_channels=1, out_channels=size1, kernel_size=5, stride=1, padding=2),                              \n",
    "            nn.ReLU(), # Activation function                 \n",
    "            nn.MaxPool2d(kernel_size=2,stride=2,padding=0)\n",
    "        ]\n",
    "        if batch_norm: conv1.append(nn.BatchNorm2d(size1))\n",
    "\n",
    "        self.conv1 = nn.Sequential(*conv1)\n",
    "\n",
    "        conv2 = [\n",
    "            nn.Conv2d(in_channels=size1,out_channels=size2, kernel_size=5,stride=1,padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2,padding=0)\n",
    "        ]\n",
    "        if batch_norm: conv2.append(nn.BatchNorm2d(size2))\n",
    "\n",
    "        self.conv2 = nn.Sequential(*conv2)    \n",
    "        \n",
    "        # Fully connected layer, Output 10 classes\n",
    "        self.out = nn.Linear(size2 * 7 * 7, 10) # Decision Layer\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.input_dim == None: self.input_dim = x.shape\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        if self.conv1_dim == None: self.conv1_dim = x.shape\n",
    "            \n",
    "        x = self.conv2(x)\n",
    "        if self.conv2_dim == None:  self.conv2_dim = x.shape\n",
    "           \n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x) \n",
    "        return output, x \n",
    "    \n",
    "cnn1 = CNN1(BATCH_NORM).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(cnn1.parameters(), lr = 0.01)   \n",
    "\n",
    "\n",
    "# summary(cnn1, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/469], Loss: 0.11753280460834503\n",
      "Epoch [1/5], Step [200/469], Loss: 0.031483471393585205\n",
      "Epoch [1/5], Step [300/469], Loss: 0.05704391375184059\n",
      "Epoch [1/5], Step [400/469], Loss: 0.05861607566475868\n",
      "Epoch [2/5], Step [100/469], Loss: 0.011963937431573868\n",
      "Epoch [2/5], Step [200/469], Loss: 0.10138247162103653\n",
      "Epoch [2/5], Step [300/469], Loss: 0.028207922354340553\n",
      "Epoch [2/5], Step [400/469], Loss: 0.040423691272735596\n",
      "Epoch [3/5], Step [100/469], Loss: 0.09230667352676392\n",
      "Epoch [3/5], Step [200/469], Loss: 0.05819518491625786\n",
      "Epoch [3/5], Step [300/469], Loss: 0.06692968308925629\n",
      "Epoch [3/5], Step [400/469], Loss: 0.053229257464408875\n",
      "Epoch [4/5], Step [100/469], Loss: 0.02532869577407837\n",
      "Epoch [4/5], Step [200/469], Loss: 0.010937328450381756\n",
      "Epoch [4/5], Step [300/469], Loss: 0.0075055635534226894\n",
      "Epoch [4/5], Step [400/469], Loss: 0.011470272205770016\n",
      "Epoch [5/5], Step [100/469], Loss: 0.02070842683315277\n",
      "Epoch [5/5], Step [200/469], Loss: 0.021890871226787567\n",
      "Epoch [5/5], Step [300/469], Loss: 0.0037095616571605206\n",
      "Epoch [5/5], Step [400/469], Loss: 0.018915856257081032\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "# Training \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # print(\"Input tensor shape:\", images.shape)  \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs, _ = cnn1(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # backpropagation \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape torch.Size([128, 1, 28, 28])\n",
      "Shape after conv1: torch.Size([128, 16, 14, 14])\n",
      "Shape after conv2: torch.Size([128, 32, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input tensor shape\", cnn1.input_dim)\n",
    "print(\"Shape after conv1:\", cnn1.conv1_dim)\n",
    "print(\"Shape after conv2:\", cnn1.conv2_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on all 10000 test images: 98.6%\n"
     ]
    }
   ],
   "source": [
    "def eval(cnn):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        cnn.eval()  # Set model evaluation mode\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "\n",
    "            outputs_tuple = cnn(images)  \n",
    "            outputs = outputs_tuple[0]  \n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0) \n",
    "            correct += (predicted == labels).sum().item() \n",
    "\n",
    "    accuracy = correct / total * 100  \n",
    "    print(f'Accuracy on all 10000 test images: {accuracy}%')\n",
    "    return accuracy\n",
    "\n",
    "accuracy = eval(cnn1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to use the measurements learned in the book\n",
    "\n",
    "\n",
    "Need to know:\n",
    "1. MEC of fully connected (decision) layer -> use NN calc\n",
    "2. The amount of bits of information arriving at that decision layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEC_decision = (1568 in features, + 1 bias) * 10 neurons = 15690 bits\n"
     ]
    }
   ],
   "source": [
    "decision_layer_in_features = cnn1.out.in_features\n",
    "decision_layer_out_features = cnn1.out.out_features\n",
    "MEC_decision_cnn1 = (decision_layer_in_features + 1) * decision_layer_out_features\n",
    "\n",
    "print(f\"MEC_decision = ({decision_layer_in_features} in features, + 1 bias) * {decision_layer_out_features} neurons = {MEC_decision_cnn1} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Manual Calculation of MEC of fully connected (decision) layer\n",
    "\n",
    "It takes in 1568 features as input, which comes from flattening the 32 channels of 7x7 feature maps output from the conv2 layer (32 * 7 * 7 = 1568).\n",
    "\n",
    "(out): Linear(in_features=1568, out_features=10, bias=True)\n",
    "\n",
    "10 neurons, each with 1568 inputs, plus a bias term\n",
    "\n",
    "MEC = (1568 + 1 bias) * 10 = **15690 bits**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The amount of bits of information arriving at that decision layer**  \n",
    " \n",
    "![alt text](def9-1.jpg \"Definition 9.1\")  \n",
    "![alt text](cor9.jpg \"Corollary 9.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G1: 0.25\n",
      "G2: 2.0\n",
      "Total compression G_total = 0.50\n",
      "\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "       BatchNorm2d-4           [-1, 16, 14, 14]              32\n",
      "            Conv2d-5           [-1, 32, 14, 14]          12,832\n",
      "              ReLU-6           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-7             [-1, 32, 7, 7]               0\n",
      "       BatchNorm2d-8             [-1, 32, 7, 7]              64\n",
      "            Linear-9                   [-1, 10]          15,690\n",
      "================================================================\n",
      "Total params: 29,034\n",
      "Trainable params: 29,034\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.36\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 0.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g1 = np.prod(cnn1.input_dim)/np.prod(cnn1.conv1_dim)\n",
    "print('G1:', g1)\n",
    "\n",
    "g2 = np.prod(cnn1.conv1_dim)/np.prod(cnn1.conv2_dim)\n",
    "print('G2:', g2)\n",
    "\n",
    "g_total_cnn1 = g1 * g2\n",
    "\n",
    "print(f'Total compression G_total = {g_total_cnn1:.2f}\\n')\n",
    "\n",
    "summary(cnn1.to('cpu'), (1, 28, 28))\n",
    "del cnn1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Manual Calculation of Convolution Compression\n",
    "Input tensor shape: torch.Size([64, 1, 28, 28])  \n",
    "Shape after conv1: torch.Size([64, 16, 14, 14])  \n",
    "Shape after conv2: torch.Size([64, 32, 7, 7])  \n",
    "\n",
    "Input tensor shape: torch.Size([64, 1, 28, 28])\n",
    "* (# inputs) = 64 {batch size}\n",
    "* (input height) = 28 {The height of the MNIST images, 28 pixels}  \n",
    "* (input width) = 28 {The width of the MNIST images, 28 pixels}  \n",
    "* (input channels) = 1 {MNIST images are grayscale, 1 input channel}  \n",
    "\n",
    "Output tensor shape:\n",
    "Shape after conv1: torch.Size([64, 16, 14, 14])\n",
    "\n",
    "G1 = (64 * 1 * 28 * 28)/(64 * 16 * 14 * 14) = **0.25**  \n",
    "G2 = (64 * 16 * 14 * 14)/(64 * 32 * 7 * 7) = **2**\n",
    "\n",
    "G_total = 0.25 * 2 = **0.5** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Attempt, using MEC and Compression (G) to tune Hyperparameters  \n",
    "I tuned hyperparameters by trying to maintain accuracy while increasing G > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self, batch_norm=True, conv1_only=False):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.conv1_only = conv1_only\n",
    "        self.input_dim = None\n",
    "        self.conv1_dim = None\n",
    "        self.conv2_dim = None\n",
    "\n",
    "        # I tuned hyperparameters by trying to increase accuracy while maintaining G > 2\n",
    "        size1 = 4\n",
    "        size2 = 15\n",
    "\n",
    "        conv1 = [\n",
    "            nn.Conv2d(in_channels=1, out_channels=size1, kernel_size=4, stride=2, padding=0),                              \n",
    "            nn.ReLU(), # Activation function                 \n",
    "            nn.MaxPool2d(kernel_size=2, stride=1,padding=1)\n",
    "        ]\n",
    "        if batch_norm: conv1.append(nn.BatchNorm2d(size1))\n",
    "\n",
    "        self.conv1 = nn.Sequential(*conv1)\n",
    "\n",
    "        conv2 = [\n",
    "            nn.Conv2d(in_channels=size1, out_channels=size2, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=1, padding=0)\n",
    "        ]\n",
    "        if batch_norm: conv2.append(nn.BatchNorm2d(size2))\n",
    "\n",
    "        self.conv2 = nn.Sequential(*conv2)\n",
    "\n",
    "\n",
    "\n",
    "        # Fully connected layer, output 10 classes\n",
    "\n",
    "        if self.conv1_only:\n",
    "            self.out = nn.Linear(size1 * 14 * 14, 10)\n",
    "        else:\n",
    "            self.out = nn.Linear(size2 * 5 * 5, 10) # Decision Layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.input_dim == None: self.input_dim = x.shape\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        if self.conv1_dim == None: self.conv1_dim = x.shape\n",
    "            \n",
    "        if not self.conv1_only:    \n",
    "            x = self.conv2(x)\n",
    "            if self.conv2_dim == None:  self.conv2_dim = x.shape\n",
    "           \n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x) \n",
    "        return output, x \n",
    "    \n",
    "cnn2 = CNN2(BATCH_NORM, conv1_only=False).to(device) # Set to True to test with only the first convolution -> results in lower accuracy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(cnn2.parameters(), lr = 0.01)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/469], Loss: 0.07444819808006287\n",
      "Epoch [1/5], Step [200/469], Loss: 0.16685014963150024\n",
      "Epoch [1/5], Step [300/469], Loss: 0.1369251012802124\n",
      "Epoch [1/5], Step [400/469], Loss: 0.1766398847103119\n",
      "Epoch [2/5], Step [100/469], Loss: 0.10923264175653458\n",
      "Epoch [2/5], Step [200/469], Loss: 0.017553744837641716\n",
      "Epoch [2/5], Step [300/469], Loss: 0.053314197808504105\n",
      "Epoch [2/5], Step [400/469], Loss: 0.03503665700554848\n",
      "Epoch [3/5], Step [100/469], Loss: 0.049106352031230927\n",
      "Epoch [3/5], Step [200/469], Loss: 0.011438676156103611\n",
      "Epoch [3/5], Step [300/469], Loss: 0.07855498790740967\n",
      "Epoch [3/5], Step [400/469], Loss: 0.035285383462905884\n",
      "Epoch [4/5], Step [100/469], Loss: 0.031362779438495636\n",
      "Epoch [4/5], Step [200/469], Loss: 0.058974526822566986\n",
      "Epoch [4/5], Step [300/469], Loss: 0.019044429063796997\n",
      "Epoch [4/5], Step [400/469], Loss: 0.03395812213420868\n",
      "Epoch [5/5], Step [100/469], Loss: 0.057196199893951416\n",
      "Epoch [5/5], Step [200/469], Loss: 0.027254633605480194\n",
      "Epoch [5/5], Step [300/469], Loss: 0.0182657353579998\n",
      "Epoch [5/5], Step [400/469], Loss: 0.051853716373443604\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # print(\"Input tensor shape:\", images.shape)  \n",
    "        images, labels = images.to(device), labels.to(device) # Move to device\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = cnn2(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # backpropagation \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape torch.Size([128, 1, 28, 28])\n",
      "Shape after conv1: torch.Size([128, 4, 14, 14])\n",
      "Shape after conv2: torch.Size([128, 15, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input tensor shape\", cnn2.input_dim)\n",
    "print(\"Shape after conv1:\", cnn2.conv1_dim)\n",
    "print(\"Shape after conv2:\", cnn2.conv2_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on all 10000 test images: 98.8%\n"
     ]
    }
   ],
   "source": [
    "accuracy_cnn2 = eval(cnn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEC_decision = (375 in features, + 1 bias) * 10 neurons = 3760 bits\n"
     ]
    }
   ],
   "source": [
    "decision_layer_in_features = cnn2.out.in_features\n",
    "decision_layer_out_features = cnn2.out.out_features\n",
    "MEC_decision_cnn2 = (decision_layer_in_features + 1) * decision_layer_out_features\n",
    "\n",
    "print(f\"MEC_decision = ({decision_layer_in_features} in features, + 1 bias) * {decision_layer_out_features} neurons = {MEC_decision_cnn2} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G1: 1.0\n",
      "G2: 2.0906666666666665\n",
      "Total compression G_total = 2.09\n",
      "\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 4, 13, 13]              68\n",
      "              ReLU-2            [-1, 4, 13, 13]               0\n",
      "         MaxPool2d-3            [-1, 4, 14, 14]               0\n",
      "       BatchNorm2d-4            [-1, 4, 14, 14]               8\n",
      "            Conv2d-5             [-1, 15, 6, 6]             975\n",
      "              ReLU-6             [-1, 15, 6, 6]               0\n",
      "         MaxPool2d-7             [-1, 15, 5, 5]               0\n",
      "       BatchNorm2d-8             [-1, 15, 5, 5]              30\n",
      "            Linear-9                   [-1, 10]           3,760\n",
      "================================================================\n",
      "Total params: 4,841\n",
      "Trainable params: 4,841\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g1 = np.prod(cnn2.input_dim)/np.prod(cnn2.conv1_dim)\n",
    "print('G1:', g1)\n",
    "\n",
    "if not cnn2.conv1_only:\n",
    "    g2 = np.prod(cnn2.conv1_dim)/np.prod(cnn2.conv2_dim)\n",
    "    print('G2:', g2)\n",
    "\n",
    "else:\n",
    "    g2 = 1\n",
    "\n",
    "g_total_cnn2 = g1 * g2\n",
    "\n",
    "print(f'Total compression G_total = {g_total_cnn2:.2f}\\n')\n",
    "\n",
    "\n",
    "summary(cnn2.to('cpu'), (1, 28, 28))\n",
    "# del cnn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced MEC_Decision from 15690 to 3760: Decreased by a factor of 4.17\n",
      "Increased G_total from 0.5 to 2.09: Increased by a factor of 4.18\n",
      "\n",
      "Accuracy: CNN1 = 98.6%, CNN2 = 98.8%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reduced MEC_Decision from {MEC_decision_cnn1} to {MEC_decision_cnn2}: Decreased by a factor of {MEC_decision_cnn1/MEC_decision_cnn2:.2f}\")\n",
    "print(f\"Increased G_total from {g_total_cnn1} to {g_total_cnn2:.2f}: Increased by a factor of {g_total_cnn2/g_total_cnn1:.2f}\\n\")\n",
    "\n",
    "print(f\"Accuracy: CNN1 = {accuracy}%, CNN2 = {accuracy_cnn2}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "First, I showed mathematically that the first attempt, CNN1 (not using the measurements in the book) was grossly overfiting and not even compressing data (G_total < 1):\n",
    "* **CNN1**\n",
    "  * MEC_Decision = 15690\n",
    "  * Compression G_total = 0.5\n",
    "  * Accuracy = 98.6%\n",
    "\n",
    "Then, I proved that the model was overfitting by creating a new model (CNN2) that was 4x smaller but with the same accuracy:\n",
    "* **CNN2**  \n",
    "  * CNN2 MEC_Decision = 3760\n",
    "  * CNN2 Compression G_total = 2.09\n",
    "  * CNN2 Accuracy = 98.8%\n",
    "\n",
    "Additional Finding:\n",
    "The first convolution (conv1) on CNN2 may just be memorizing the data, as G1 = 1. You can test with only conv1 by setting (conv1_only=True) in the CNN2 cell above.  \n",
    "Removing the second convolution on CNN2 () gives the following:  \n",
    "* **CNN2 First Convolution Only**\n",
    "  * MEC_Decision = 7850\n",
    "  * Compression G_total = 1.00\n",
    "  * Only Accuracy = 97.69%\n",
    "\n",
    "Finally, The main takeaway is that by using the measurements in the book I achieved the following:  \n",
    "1. **Decreased Decision MEC by a factor of 4.17**::\n",
    "   * Reduced MEC_Decision from 15690 to 3760\n",
    "2. **Increased Compression by a factor of 4.18**:\n",
    "   * Increased G_total from 0.5 to 2.09\n",
    "3. Maintained Testing Accuracy while being **≈4x smaller**:\n",
    "   * **Accuracy: CNN1 = 98.6%, CNN2 = 98.8%**\n",
    "\n",
    "\n",
    "#### Main Takeaway\n",
    "Using Compression (G) to guide Hyperparameter tuning resulted in a CNN model **7.8x smaller** with **Same Accuracy**:\n",
    "  * PyTorch Estimated Total Size: **CNN1 = 0.47MB vs CNN2 = 0.06MB**\n",
    "  * Testing Accuracy: **CNN1 = 98.6% vs CNN2 = 98.8%**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P.S. Turning off batch normalization (batch_norm=False) results in the **exact same** accuracy: CNN1_no_batch = 97.91%, CNN2_no_batch = 97.91%.  \n",
    "Also, turning off batch normalization shows that CNN2 is faster to train (by 1-2 seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
